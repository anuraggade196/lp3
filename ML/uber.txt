import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import zscore

data = pd.read_csv('uber.csv')
data.head()



print("Missing values per column:\n", data.isnull().sum())

data = data.dropna()

# Convert data types if necessary (e.g., ensure numeric columns)
data = data.astype({'pickup_latitude': 'float64', 'pickup_longitude': 'float64',
                    'dropoff_latitude': 'float64', 'dropoff_longitude': 'float64', 
                    'fare_amount': 'float64'})

# Feature Engineering: Calculate distance between pickup and dropoff using Haversine formula
def haversine_distance(lat1, lon1, lat2, lon2):
    R = 6371  # Radius of Earth in kilometers
    phi1 = np.radians(lat1)
    phi2 = np.radians(lat2)
    delta_phi = np.radians(lat2 - lat1)
    delta_lambda = np.radians(lon2 - lon1)
    
    a = np.sin(delta_phi / 2) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2) ** 2
    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))

data['distance_km'] = haversine_distance(data['pickup_latitude'], data['pickup_longitude'],
                                         data['dropoff_latitude'], data['dropoff_longitude'])

# Remove unrealistic distances or fares (filter out negative or zero fares and distances)
data = data[(data['distance_km'] > 0) & (data['fare_amount'] > 0)]

# Display summary of data
data.describe()



# Use z-score to identify outliers
z_scores = zscore(data[['fare_amount', 'distance_km']])
abs_z_scores = np.abs(z_scores)
filtered_entries = (abs_z_scores < 3).all(axis=1)  # Keep data within 3 standard deviations
data = data[filtered_entries]

# Visualize the distribution of 'fare_amount' and 'distance_km' to check for remaining outliers
sns.boxplot(data['fare_amount'])
plt.title("Fare Amount Boxplot")
plt.show()

sns.boxplot(data['distance_km'])
plt.title("Distance (km) Boxplot")
plt.show()



# Check correlation between features
correlation_matrix = data[['fare_amount', 'distance_km']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()



# Define features (X) and target (y)
X = data[['distance_km']]
y = data['fare_amount']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Linear Regression
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)
y_pred_linear = linear_model.predict(X_test)

# Random Forest Regression
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)



# Function to calculate evaluation metrics
def evaluate_model(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    return mse, rmse, r2

# Linear Regression Metrics
mse_linear, rmse_linear, r2_linear = evaluate_model(y_test, y_pred_linear)
print(f"Linear Regression - MSE: {mse_linear:.2f}, RMSE: {rmse_linear:.2f}, R2: {r2_linear:.2f}")

# Random Forest Metrics
mse_rf, rmse_rf, r2_rf = evaluate_model(y_test, y_pred_rf)
print(f"Random Forest - MSE: {mse_rf:.2f}, RMSE: {rmse_rf:.2f}, R2: {r2_rf:.2f}")

# Compare model performances
results = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest'],
    'MSE': [mse_linear, mse_rf],
    'RMSE': [rmse_linear, rmse_rf],
    'R2': [r2_linear, r2_rf]
})
print(results)